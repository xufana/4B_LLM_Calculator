{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code and further work is inspired by: [Liu T., Hsiang B. 2023](https://arxiv.org/pdf/2305.14201.pdf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Dataset Generation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we do not really have anything to test our theories on, let us create a datset we will use. Obviously, we will create a lot of addition examples, but there is a thing. We are also going to generate pairs of numbers and the result of their *subtraction*, because these two operations are inverse operations, which means addition undoes subtraction and subtraction undoes addition. As a human being that is why I find subtraction examples usefull too. The human logic might not work well with LLM logic, but that is what we are here to try and test.\n",
        "\n",
        "We will train two different models with and without this part of dataset and compare their results to confirm or disprove my theory. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset mainly follows Dolly-2.0 style (instruction dataset). It has four keys: 'instruction', 'input', 'output', 'answer'."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*uncomment this if you have not generated the data yet*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: 568000\n",
            "Subtraction: 56800\n",
            "Total: 624800\n",
            "Arithmetic dataset generated!\n",
            "Total: 624800\n",
            "Dataset generated!\n"
          ]
        }
      ],
      "source": [
        "# !python3 dataset_generator.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TD4h4uM2lKtk"
      },
      "source": [
        "# 1. Prompt Engineering\n",
        "\n",
        "First, and the most obvious option we have is to choose a small model and set a series of experiments on how a model can work just as \"out-of-the-box\".\n",
        "\n",
        "The model we are going to test is [RedPajama-INCITE-3B](https://www.together.xyz/blog/redpajama-3b-updates), because it is an open-source model based on LLaMA, and it it relatively small. It fits perfectly under the constraint of 4B parameters, and also can be rapidly trained with pretty low requirements (and I am a broke student). Its another strength is the tokenisation inherited from LLaMa: it separates the numbers into a set of individual digits, while other models can interpret '232' as '2' and '32' or as '23' and '2' as well (you can see the full comparison within the different models in this work [Nogueira et. al. 2021](https://arxiv.org/pdf/2102.13019.pdf)). Yes, it might be a drawback with something more 'solid', like years, dates, etc. but this is not our case, so it would not affect our performance in that way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cTM25Cja8dc"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kNprhQ54bAKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MIN_TRANSFORMERS_VERSION = '4.25.1'\n",
        "\n",
        "# check transformers version\n",
        "assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J28RaI9QcQPI"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\", torch_dtype=torch.float16)\n",
        "model = model.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK0ZB_PJbPNw",
        "outputId": "ab46f340-c13f-471f-c0ac-2bb19d01673f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: fifty seven\n",
            "Q:\n"
          ]
        }
      ],
      "source": [
        "# infer\n",
        "prompt = \"Q: one plus three\\nA: four\\nQ: twenty plus forty two\\nA: sixty two\\nQ: twelve plus thirty three\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=7, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSIjxltkcR0k",
        "outputId": "b203d737-8fda-4653-c136-b9cbfba1626f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: 45\n",
            "Q: -1 plus 4\n"
          ]
        }
      ],
      "source": [
        "# infer\n",
        "prompt = \"Q: 1 plus 3\\nA: 4\\nQ: 20 plus 42\\nA: 62 \\nQ: 12 plus 33\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=10, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNbohSC7K6f"
      },
      "source": [
        "# 2. Fine-Tuning and Test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, we are going to set everything up, and if you want to do some fine-tuning on your own, please run the code below. It was run in colab originally, so if you are using a local machine, you can skip some steps and start generating the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/xufana/4B_LLM_Calculator.git\n",
        "%cd /content/4B_LLM_Calculator\n",
        "! pip install -r requirements.txt\n",
        "! python3 dataset_generator.py --add_volume 100 --sub_volume 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*uncomment if you want to use wandb*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default config is \n",
        "```\n",
        "    base_model: str = \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
        "\n",
        "    batch_size: int = 16,\n",
        "    micro_batch_size: int = 4,\n",
        "    num_epochs: int = 1,\n",
        "    learning_rate: float = 2e-4,\n",
        "    cutoff_len: int = 512,\n",
        "    val_set_size: int = 0,\n",
        "    \n",
        "    # lora hyperparams\n",
        "    lora_r: int = 8,\n",
        "    lora_alpha: int = 32,\n",
        "    lora_dropout: float = 0.05,\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code took 8 hours to fine-tune on one T4 and used about 10.2Gb VRAM. I used colab pro, but all in all it seems to fit perfectly within the free version. The version with (`batch_size = 128`) and (`micro_batch_size = 16`) took about 12Gb VRAM, so it should work too as well.\n",
        "\n",
        "Important to notice, I set (`lora_r = 8`), while the authors of the GOAT used (`lora_r = 16`), but I cut it more to fit into colab (I am still a poor student all in all)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python3 lora_training.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`trainable params: 2621440 || all params: 2778485760 || trainable%: 0.09434779323828531`\n",
        "\n",
        "nice to see"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate some more addition data to test the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: 56800\n",
            "Total: 56800\n",
            "Adding instructions and noise\n",
            "Total: 56800\n",
            "Dataset generated!\n"
          ]
        }
      ],
      "source": [
        "! python3 dataset_generator.py --dataset_name \"test.json\" --need_sub False --add_volume 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "CUDA SETUP: Loading binary /Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
            "dlopen(/Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/xufana/workspace/4B_LLM_Calculator/4B_LLM_Calculator/.venv/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"mps\")\n",
        "peft_model_id = \"xufana/RedPajama-3B-Arithmetics\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map={\"\": device})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p = 0.75,\n",
        "        top_k = 40,\n",
        "        num_beams = 4,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        }
      ],
      "source": [
        "prompt = \"Q: Calculate 123 + 345\\nA:\"\n",
        "batch = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  output_tokens = model.generate(**batch, generation_config=generation_config, max_new_tokens=10)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=False))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model compression [TBD]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The idea here is to take an existing solution, which is slightly bigger than we need, but we can try and compress it. I will take a model Goat-7B by [Liu T., Hsiang B. 2023](https://arxiv.org/pdf/2305.14201.pdf) ([HF link for the weights](https://huggingface.co/tiedong/goat-lora-7b)). It was already fine-tuned for arithmetics tasks, and especially addition up to 16-digits, so basically it is ready, but just a little bit bigger than required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "54bdfb8430d8cb2bab15c40e5aa9036856784170e7ae7be9a0b7ad5d6e0aa2da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
