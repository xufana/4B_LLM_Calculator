{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code and further work is inspired by: [Liu T., Hsiang B. 2023](https://arxiv.org/pdf/2305.14201.pdf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Dataset Generation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we do not really have anything to test our theories on, let us create a datset we will use. Obviously, we will create a lot of addition examples, but there is a thing. We are also going to generate pairs of numbers and the result of their *subtraction*, because these two operations are inverse operations, which means addition undoes subtraction and subtraction undoes addition. As a human being that is why I find subtraction examples usefull too. The human logic might not work well with LLM logic, but that is what we are here to try and test.\n",
        "\n",
        "We will train two different models with and without this part of dataset and compare their results to confirm or disprove my theory. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset mainly follows Dolly-2.0 style (instruction dataset). It has four keys: 'instruction', 'input', 'output', 'answer'."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*uncomment this if you have not generated the data yet*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: 568000\n",
            "Subtraction: 56800\n",
            "Total: 624800\n",
            "Arithmetic dataset generated!\n",
            "Total: 624800\n",
            "Dataset generated!\n"
          ]
        }
      ],
      "source": [
        "# !python3 dataset_generator.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TD4h4uM2lKtk"
      },
      "source": [
        "# 1. Prompt Engineering\n",
        "\n",
        "First, and the most obvious option we have is to choose a small model and set a series of experiments on how a model can work just as \"out-of-the-box\".\n",
        "\n",
        "The model we are going to test is [RedPajama-INCITE-3B](https://www.together.xyz/blog/redpajama-3b-updates), because it is an open-source model based on LLaMA, and it it relatively small. It fits perfectly under the constraint of 4B parameters, and also can be rapidly trained with pretty low requirements (and I am a broke student). Its another strength is the tokenisation inherited from LLaMa: it separates the numbers into a set of individual digits, while other models can interpret '232' as '2' and '32' or as '23' and '2' as well (you can see the full comparison within the different models in this work [Nogueira et. al. 2021](https://arxiv.org/pdf/2102.13019.pdf)). Yes, it might be a drawback with something more 'solid', like years, dates, etc. but this is not our case, so it would not affect our performance in that way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cTM25Cja8dc"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kNprhQ54bAKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MIN_TRANSFORMERS_VERSION = '4.25.1'\n",
        "\n",
        "# check transformers version\n",
        "assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J28RaI9QcQPI"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\", torch_dtype=torch.float16)\n",
        "model = model.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK0ZB_PJbPNw",
        "outputId": "ab46f340-c13f-471f-c0ac-2bb19d01673f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: fifty seven\n",
            "Q:\n"
          ]
        }
      ],
      "source": [
        "# infer\n",
        "prompt = \"Q: one plus three\\nA: four\\nQ: twenty plus forty two\\nA: sixty two\\nQ: twelve plus thirty three\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=7, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSIjxltkcR0k",
        "outputId": "b203d737-8fda-4653-c136-b9cbfba1626f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: 45\n",
            "Q: -1 plus 4\n"
          ]
        }
      ],
      "source": [
        "# infer\n",
        "prompt = \"Q: 1 plus 3\\nA: 4\\nQ: 20 plus 42\\nA: 62 \\nQ: 12 plus 33\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=10, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNbohSC7K6f"
      },
      "source": [
        "# 2. Fine-Tuning and Test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, we are going to set everything up, and if you want to do some fine-tuning on your own, please run the code below. It was run in colab originally, so if you are using a local machine, you can skip some steps and start generating the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/xufana/4B_LLM_Calculator.git\n",
        "%cd /content/4B_LLM_Calculator\n",
        "! pip install -r requirements.txt\n",
        "! python3 dataset_generator.py --add_volume 100 --sub_volume 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*uncomment if you want to use wandb*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default config is \n",
        "```\n",
        "    base_model: str = \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
        "\n",
        "    batch_size: int = 16,\n",
        "    micro_batch_size: int = 4,\n",
        "    num_epochs: int = 1,\n",
        "    learning_rate: float = 2e-4,\n",
        "    cutoff_len: int = 512,\n",
        "    val_set_size: int = 0,\n",
        "    \n",
        "    # lora hyperparams\n",
        "    lora_r: int = 8,\n",
        "    lora_alpha: int = 32,\n",
        "    lora_dropout: float = 0.05,\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code took 8 hours to fine-tune on one T4 and used about 10.2Gb VRAM. I used colab pro, but all in all it seems to fit perfectly within the free version. The version with (`batch_size = 128`) and (`micro_batch_size = 16`) took about 12Gb VRAM, so it should work too as well.\n",
        "\n",
        "Important to notice, I set (`lora_r = 8`), while the authors of the GOAT used (`lora_r = 16`), but I cut it more to fit into colab (I am still a poor student all in all)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python3 lora_training.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`trainable params: 2621440 || all params: 2778485760 || trainable%: 0.09434779323828531`\n",
        "\n",
        "nice to see"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate some more addition data to test the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: 56800\n",
            "Total: 56800\n",
            "Adding instructions and noise\n",
            "Total: 56800\n",
            "Dataset generated!\n"
          ]
        }
      ],
      "source": [
        "! python3 dataset_generator.py --dataset_name \"test.json\" --need_sub False --add_volume 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEMPERATURE = 0.1\n",
        "TOP_P = 0.75\n",
        "TOP_K = 40\n",
        "NUM_BEAMS = 4\n",
        "MAX_NEW_TOKENS = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load test dataset for the inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "from utils.prompter import Prompter\n",
        "from datasets import load_dataset\n",
        "\n",
        "#data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
        "data = load_dataset(\"json\", data_files=\"data/test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL INITIALISATION\n",
        "\n",
        "base_model = \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\"\n",
        "lora_weights = \"xufana/RedPajama-3B-Addition\"\n",
        "load_8bit = True\n",
        "\n",
        "prompter = Prompter()\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pajama = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, \n",
        "    load_in_8bit=load_8bit,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    pajama,\n",
        "    lora_weights,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={'': 0}\n",
        ")\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': '1413054210151409278', 'output': '490200312900021509 + 922853897251387769 = 1413054210151409278', 'input': '490200312900021509 + 922853897251387769', 'instruction': '490200312900021509+922853897251387769='}\n",
            "{'answer': '8906300461271423', 'output': '1252050841 + 8906299209220582 = 8906300461271423', 'input': '1252050841 + 8906299209220582', 'instruction': 'compute 1252050841+8906299209220582'}\n"
          ]
        }
      ],
      "source": [
        "# ANSWERS GENERATION\n",
        "\n",
        "answers = []\n",
        "for row in data[\"train\"]:\n",
        "    instruction = row[\"instruction\"]\n",
        "    prompt = prompter.generate_prompt(instruction)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        num_beams=NUM_BEAMS,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "        )\n",
        "    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)\n",
        "    answer = prompter.get_response(output)\n",
        "\n",
        "    try:\n",
        "        answer = int(answer)\n",
        "    except:\n",
        "        answer = int(re.search(r'\\d+', output).group())\n",
        "\n",
        "    \n",
        "        \n",
        "    answers.append({\"numbers\": row[\"input\"], \"answer\": answer, \"ground_truth\": int(row[\"answer\"])})\n",
        "\n",
        "with open(\"./results\" + \"addition_only_answers.json\", \"w\") as f:\n",
        "        json.dump(answers, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_true = [entry['ground_truth'] for entry in data]\n",
        "y_pred = [entry['answer'] for entry in data]\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print('This is how well I have traind my model:', accuracy)\n",
        "print('idk why it ended up that bad...')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model compression [TBD]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The idea here is to take an existing solution, which is slightly bigger than we need, but we can try and compress it. I will take a model Goat-7B by [Liu T., Hsiang B. 2023](https://arxiv.org/pdf/2305.14201.pdf) ([HF link for the weights](https://huggingface.co/tiedong/goat-lora-7b)). It was already fine-tuned for arithmetics tasks, and especially addition up to 16-digits, so basically it is ready, but just a little bit bigger than required."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "54bdfb8430d8cb2bab15c40e5aa9036856784170e7ae7be9a0b7ad5d6e0aa2da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
